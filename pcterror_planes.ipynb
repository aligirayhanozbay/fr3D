{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6350a0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import h5py\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.path as mplPath\n",
    "\n",
    "from fr3D.train.utils import setup_datasets\n",
    "from fr3D.data.utils import get_normalization_type\n",
    "from fr3D.models import ConvAutoencoder, ConvAutoencoderCGAN, ConvAutoencoderC\n",
    "from fr3D.utils import mape_with_threshold\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb8a321",
   "metadata": {},
   "outputs": [],
   "source": [
    "Re = 500\n",
    "expt_variables = ['Pressure']\n",
    "\n",
    "dataset_path = f'/fr3D/postprocessed/annulus_64_plane.h5'\n",
    "\n",
    "#ConvAutoencoderC\n",
    "experiment_configs = {'Pressure':f'/fr3D/configs/training/ConvAutoencoderC_PIV.json'}\n",
    "weights_paths = {'Pressure':f'/storage/weights500Old/ConvAutoencoderC_PIV_Annulus64/ConvAutoencoderC_PIV_Annulus64.h5'}\n",
    "\n",
    "\n",
    "\n",
    "datasetf = h5py.File(dataset_path,'r')\n",
    "\n",
    "shuf_buf = 1\n",
    "\n",
    "train_datasets = {}\n",
    "test_datasets = {}\n",
    "sensor_shapes = {}\n",
    "full_field_shapes = {}\n",
    "normalizers = {}\n",
    "\n",
    "for expt_variable in experiment_configs:\n",
    "    config = json.load(open(experiment_configs[expt_variable],'r'))\n",
    "    train_datasets[expt_variable], test_datasets[expt_variable] = setup_datasets(config, dataset_path, shuf_buf, case_names=True, evaluation=True)\n",
    "    sensor_shapes[expt_variable] = train_datasets[expt_variable].element_spec[0][0].shape\n",
    "    full_field_shapes[expt_variable] = train_datasets[expt_variable].element_spec[0][1].shape\n",
    "    normalizers[expt_variable] = get_normalization_type(config['dataset']['node_configurations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89aad0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "\n",
    "for v in expt_variables:\n",
    "    config = json.load(open(experiment_configs[v],'r'))\n",
    "    model = ConvAutoencoderC(dense_input_units=sensor_shapes[v][1],\n",
    "                             autoencoder_input_shape=full_field_shapes[v][1:],\n",
    "                             **config['model'])\n",
    "    loss_fn = \"mse\"#tf.keras.losses.get(config['training']['loss'])\n",
    "    model.compile(l_optimizer= tf.keras.optimizers.get(config['training']['l_optimizer']),\n",
    "                  loss=loss_fn,\n",
    "                  optimizer = tf.keras.optimizers.get(config['training']['ae_optimizer']),\n",
    "                  metrics = config['training'].get('metrics', None))\n",
    "    model.load_weights(weights_paths[v])\n",
    "    models[v] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6bc535",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def get_predictions(model, inp, target):\n",
    "    return model(inp, autoencode=False), model(target, autoencode=True)\n",
    "\n",
    "def undo_normalization(normalizer, norm_param, pred, ae_pred, target):\n",
    "    ntarget = normalizer.undo(target, norm_param[:,0,:])\n",
    "    npred = normalizer.undo(pred, norm_param[:,0,:])\n",
    "    nae_pred = normalizer.undo(ae_pred, norm_param[:,0,:])\n",
    "    return npred, nae_pred, ntarget\n",
    "\n",
    "@tf.function\n",
    "def check_in_box(coords, bounds_lower, bounds_upper):\n",
    "    newshape = tf.concat([tf.ones((tf.rank(coords)-1,), dtype=tf.int32), tf.shape(bounds_lower)[:1]],0)\n",
    "    bounds_lower_r = tf.reshape(bounds_lower, newshape)\n",
    "    bounds_upper_r = tf.reshape(bounds_upper, newshape)\n",
    "    lower_cond = tf.reduce_all(coords > bounds_lower, axis=-1)\n",
    "    upper_cond = tf.reduce_all(coords < bounds_upper, axis=-1)\n",
    "    return tf.logical_and(lower_cond, upper_cond)\n",
    "\n",
    "def compute_metrics(coords, v, mapes, umapes, mses, umses, pred, target, npred, ntarget):\n",
    "    lower_box_bounds = tf.constant([-3.0,-3.0,-0.1])\n",
    "    upper_box_bounds = tf.constant([8.0,3.0,10.1])\n",
    "    in_box = check_in_box(coords, lower_box_bounds, upper_box_bounds)\n",
    "    in_box_r = tf.reshape(in_box, tf.concat([tf.shape(in_box), tf.ones((tf.rank(p_npred) - tf.rank(in_box),), tf.int32)], 0))\n",
    "    in_box_rf = tf.cast(in_box_r, tf.float32)\n",
    "    \n",
    "    _lf =tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE)\n",
    "    \n",
    "    mapes[v].append(\n",
    "        mape_with_threshold(npred, ntarget, pcterror_threshold=100.0, max_magnitude_threshold=0.03, sample_weights=in_box_rf, axis=tf.range(1, tf.rank(in_box_rf)))\n",
    "    )\n",
    "    umapes[v].append(\n",
    "        mape_with_threshold(pred, target, pcterror_threshold=100.0, max_magnitude_threshold=0.03, sample_weights=in_box_rf, axis=tf.range(1, tf.rank(in_box_rf)))\n",
    "    )\n",
    "    mses[v].append(tf.reduce_mean(_lf(npred, ntarget, in_box_rf), axis=tf.range(1, tf.rank(in_box_rf)-1)))\n",
    "    umses[v].append(tf.reduce_mean(_lf(pred, target, in_box_rf), axis=tf.range(1, tf.rank(in_box_rf)-1)))\n",
    "\n",
    "\n",
    "dataset_iterators = {v: iter(test_datasets[v]) for v in test_datasets}\n",
    "\n",
    "pred_forces = []\n",
    "ae_pred_forces = []\n",
    "target_forces = []\n",
    "\n",
    "pred_mapes = {k:[] for k in dataset_iterators.keys()}\n",
    "pred_unnormalized_mapes = {k:[] for k in dataset_iterators.keys()}\n",
    "pred_mses = {k:[] for k in dataset_iterators.keys()}\n",
    "pred_unnormalized_mses = {k:[] for k in dataset_iterators.keys()}\n",
    "\n",
    "ae_pred_mapes = {k:[] for k in dataset_iterators.keys()}\n",
    "ae_pred_unnormalized_mapes = {k:[] for k in dataset_iterators.keys()}\n",
    "ae_pred_mses = {k:[] for k in dataset_iterators.keys()}\n",
    "ae_pred_unnormalized_mses = {k:[] for k in dataset_iterators.keys()}\n",
    "\n",
    "for pdata in zip(*dataset_iterators.values()):\n",
    "    (p_inp, p_target, p_norm_param), case_name = pdata[0]\n",
    "    \n",
    "    coords = tf.convert_to_tensor(np.stack([datasetf[c.decode()]['full_field_coords'] for c in case_name.numpy()],0))\n",
    "    \n",
    "    p_pred, p_ae_pred = get_predictions(models['Pressure'], p_inp, p_target)\n",
    "    p_npred, p_nae_pred, p_ntarget = undo_normalization(normalizers['Pressure'], p_norm_param, p_pred, p_ae_pred, p_target)\n",
    "   \n",
    "    pred_pressures = p_npred[...,0]\n",
    "    ae_pred_pressures = p_nae_pred[...,0]\n",
    "    target_pressures = p_ntarget[...,0]\n",
    "    \n",
    "    compute_metrics(coords, 'Pressure', pred_mapes, pred_unnormalized_mapes, pred_mses, pred_unnormalized_mses,\n",
    "                       p_pred, p_target, p_npred, p_ntarget)\n",
    "    \n",
    "    compute_metrics(coords, 'Pressure', ae_pred_mapes, ae_pred_unnormalized_mapes, ae_pred_mses, ae_pred_unnormalized_mses,\n",
    "                       p_ae_pred, p_target, p_nae_pred, p_ntarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa6262b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mapes = {k:tf.concat(pred_mapes[k],0) for k in dataset_iterators.keys()}\n",
    "pred_unnormalized_mapes = {k:tf.concat(pred_unnormalized_mapes[k],0) for k in dataset_iterators.keys()}\n",
    "pred_mses = {k:tf.concat(pred_mses[k],0) for k in dataset_iterators.keys()}\n",
    "pred_unnormalized_mses = {k:tf.concat(pred_unnormalized_mses[k],0) for k in dataset_iterators.keys()}\n",
    "\n",
    "ae_pred_mapes = {k:tf.concat(ae_pred_mapes[k],0) for k in dataset_iterators.keys()}\n",
    "ae_pred_unnormalized_mapes = {k:tf.concat(ae_pred_unnormalized_mapes[k],0) for k in dataset_iterators.keys()}\n",
    "ae_pred_mses = {k:tf.concat(ae_pred_mses[k],0) for k in dataset_iterators.keys()}\n",
    "ae_pred_unnormalized_mses = {k:tf.concat(ae_pred_unnormalized_mses[k],0) for k in dataset_iterators.keys()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892c7f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(v, s, mapes, unnormalized_mapes, mses, unnormalized_mses):\n",
    "    print(f\">> {v} {s}\")\n",
    "    print(f'MAPE: {tf.reduce_mean(mapes[v]).numpy()}')\n",
    "    print(f'Unnormalized MAPE: {tf.reduce_mean(unnormalized_mapes[v]).numpy()}')\n",
    "    print(f'MSE: {tf.reduce_mean(mses[v]).numpy()}')\n",
    "    print(f'Unnormalized MSE: {tf.reduce_mean(unnormalized_mses[v]).numpy()}')\n",
    "    print('')\n",
    "\n",
    "print('===Overall metrics===')\n",
    "\n",
    "print_metrics('Pressure', 'Pred', pred_mapes, pred_unnormalized_mapes, pred_mses, pred_unnormalized_mses)\n",
    "print_metrics('Pressure', 'AE Pred', ae_pred_mapes, ae_pred_unnormalized_mapes, ae_pred_mses, ae_pred_unnormalized_mses)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
