{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6350a0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import h5py\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.path as mplPath\n",
    "\n",
    "from fr3D.train.utils import setup_datasets\n",
    "from fr3D.data.utils import get_normalization_type\n",
    "from fr3D.models import ConvAutoencoder, ConvAutoencoderCGAN, ConvAutoencoderC\n",
    "from fr3D.utils import mape_with_threshold\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2eb8a321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training geometries: ('shape_69', 'shape_329', 'shape_46', 'shape_241', 'shape_239', 'shape_228', 'shape_163', 'shape_63', 'shape_137', 'shape_102', 'shape_157', 'shape_197', 'shape_52', 'shape_213', 'shape_84', 'shape_113', 'shape_58', 'shape_59', 'shape_146', 'shape_206', 'shape_331', 'shape_8', 'shape_18', 'shape_236', 'shape_43', 'shape_15', 'shape_23', 'shape_92', 'shape_135', 'shape_50', 'shape_14', 'shape_255', 'shape_75', 'shape_183', 'shape_89', 'shape_12', 'shape_374', 'shape_37', 'shape_218', 'shape_150', 'shape_31', 'shape_216', 'shape_13', 'shape_90', 'shape_182', 'shape_159', 'shape_67', 'shape_170', 'shape_358', 'shape_147', 'shape_17', 'shape_110', 'shape_152', 'shape_224', 'shape_130', 'shape_60', 'shape_126', 'shape_369', 'shape_22', 'shape_83', 'shape_338', 'shape_97', 'shape_33', 'shape_237', 'shape_325', 'shape_39', 'shape_244', 'shape_41', 'shape_355', 'shape_94', 'shape_61', 'shape_234', 'shape_34', 'shape_29', 'shape_95', 'shape_339', 'shape_55', 'shape_21', 'shape_9', 'shape_343')\n",
      "Test geometries: ('shape_361', 'shape_74', 'shape_220', 'shape_186', 'shape_107', 'shape_327', 'shape_160', 'shape_11', 'shape_164', 'shape_78', 'shape_86', 'shape_54', 'shape_76', 'shape_68', 'shape_16', 'shape_346', 'shape_48', 'shape_149', 'shape_88', 'shape_326')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-05 23:54:38.693450: W tensorflow_io/core/kernels/audio_video_mp3_kernels.cc:271] libmp3lame.so.0 or lame functions are not available\n",
      "2023-01-05 23:54:38.693619: I tensorflow_io/core/kernels/cpu_check.cc:128] Your CPU supports instructions that this TensorFlow IO binary was not compiled to use: AVX2 FMA\n",
      "2023-01-05 23:54:38.850506: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-05 23:54:39.315850: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 37699 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:43:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "Re = 500\n",
    "expt_variables = ['Pressure']\n",
    "\n",
    "dataset_path = f'/fr3D/postprocessed/annulus_64_plane.h5'\n",
    "\n",
    "#ConvAutoencoderC\n",
    "experiment_configs = {'Pressure':f'/fr3D/configs/training/ConvAutoencoderC_PIV.json'}\n",
    "weights_paths = {'Pressure':f'/storage/weights500Old/ConvAutoencoderC_PIV_Annulus64/ConvAutoencoderC_PIV_Annulus64.h5'}\n",
    "\n",
    "\n",
    "\n",
    "datasetf = h5py.File(dataset_path,'r')\n",
    "\n",
    "shuf_buf = 1\n",
    "\n",
    "train_datasets = {}\n",
    "test_datasets = {}\n",
    "sensor_shapes = {}\n",
    "full_field_shapes = {}\n",
    "normalizers = {}\n",
    "\n",
    "for expt_variable in experiment_configs:\n",
    "    config = json.load(open(experiment_configs[expt_variable],'r'))\n",
    "    train_datasets[expt_variable], test_datasets[expt_variable] = setup_datasets(config, dataset_path, shuf_buf, case_names=True, evaluation=True)\n",
    "    sensor_shapes[expt_variable] = train_datasets[expt_variable].element_spec[0][0].shape\n",
    "    full_field_shapes[expt_variable] = train_datasets[expt_variable].element_spec[0][1].shape\n",
    "    normalizers[expt_variable] = get_normalization_type(config['dataset']['node_configurations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89aad0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "\n",
    "for v in expt_variables:\n",
    "    config = json.load(open(experiment_configs[v],'r'))\n",
    "    model = ConvAutoencoderC(dense_input_units=sensor_shapes[v][1],\n",
    "                             autoencoder_input_shape=full_field_shapes[v][1:],\n",
    "                             **config['model'])\n",
    "    loss_fn = \"mse\"#tf.keras.losses.get(config['training']['loss'])\n",
    "    model.compile(l_optimizer= tf.keras.optimizers.get(config['training']['l_optimizer']),\n",
    "                  loss=loss_fn,\n",
    "                  optimizer = tf.keras.optimizers.get(config['training']['ae_optimizer']),\n",
    "                  metrics = config['training'].get('metrics', None))\n",
    "    model.load_weights(weights_paths[v])\n",
    "    models[v] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc6bc535",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def get_predictions(model, inp, target):\n",
    "    return model(inp, autoencode=False), model(target, autoencode=True)\n",
    "\n",
    "def undo_normalization(normalizer, norm_param, pred, ae_pred, target):\n",
    "    ntarget = normalizer.undo(target, norm_param[:,0,:])\n",
    "    npred = normalizer.undo(pred, norm_param[:,0,:])\n",
    "    nae_pred = normalizer.undo(ae_pred, norm_param[:,0,:])\n",
    "    return npred, nae_pred, ntarget\n",
    "\n",
    "@tf.function\n",
    "def check_in_box(coords, bounds_lower, bounds_upper):\n",
    "    newshape = tf.concat([tf.ones((tf.rank(coords)-1,), dtype=tf.int32), tf.shape(bounds_lower)[:1]],0)\n",
    "    bounds_lower_r = tf.reshape(bounds_lower, newshape)\n",
    "    bounds_upper_r = tf.reshape(bounds_upper, newshape)\n",
    "    lower_cond = tf.reduce_all(coords > bounds_lower, axis=-1)\n",
    "    upper_cond = tf.reduce_all(coords < bounds_upper, axis=-1)\n",
    "    return tf.logical_and(lower_cond, upper_cond)\n",
    "\n",
    "def compute_metrics(coords, v, mapes, umapes, mses, umses, pred, target, npred, ntarget):\n",
    "    lower_box_bounds = tf.constant([-3.0,-3.0,-0.1])\n",
    "    upper_box_bounds = tf.constant([8.0,3.0,10.1])\n",
    "    in_box = check_in_box(coords, lower_box_bounds, upper_box_bounds)\n",
    "    in_box_r = tf.reshape(in_box, tf.concat([tf.shape(in_box), tf.ones((tf.rank(p_npred) - tf.rank(in_box),), tf.int32)], 0))\n",
    "    in_box_rf = tf.cast(in_box_r, tf.float32)\n",
    "    \n",
    "    _lf =tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE)\n",
    "    \n",
    "    mapes[v].append(\n",
    "        mape_with_threshold(npred, ntarget, pcterror_threshold=100.0, max_magnitude_threshold=0.03, sample_weights=in_box_rf, axis=tf.range(1, tf.rank(in_box_rf)))\n",
    "    )\n",
    "    umapes[v].append(\n",
    "        mape_with_threshold(pred, target, pcterror_threshold=100.0, max_magnitude_threshold=0.03, sample_weights=in_box_rf, axis=tf.range(1, tf.rank(in_box_rf)))\n",
    "    )\n",
    "    mses[v].append(tf.reduce_mean(_lf(npred, ntarget, in_box_rf), axis=tf.range(1, tf.rank(in_box_rf)-1)))\n",
    "    umses[v].append(tf.reduce_mean(_lf(pred, target, in_box_rf), axis=tf.range(1, tf.rank(in_box_rf)-1)))\n",
    "\n",
    "\n",
    "dataset_iterators = {v: iter(test_datasets[v]) for v in test_datasets}\n",
    "\n",
    "pred_forces = []\n",
    "ae_pred_forces = []\n",
    "target_forces = []\n",
    "\n",
    "pred_mapes = {k:[] for k in dataset_iterators.keys()}\n",
    "pred_unnormalized_mapes = {k:[] for k in dataset_iterators.keys()}\n",
    "pred_mses = {k:[] for k in dataset_iterators.keys()}\n",
    "pred_unnormalized_mses = {k:[] for k in dataset_iterators.keys()}\n",
    "\n",
    "ae_pred_mapes = {k:[] for k in dataset_iterators.keys()}\n",
    "ae_pred_unnormalized_mapes = {k:[] for k in dataset_iterators.keys()}\n",
    "ae_pred_mses = {k:[] for k in dataset_iterators.keys()}\n",
    "ae_pred_unnormalized_mses = {k:[] for k in dataset_iterators.keys()}\n",
    "\n",
    "for pdata in zip(*dataset_iterators.values()):\n",
    "    (p_inp, p_target, p_norm_param), case_name = pdata[0]\n",
    "    \n",
    "    coords = tf.convert_to_tensor(np.stack([datasetf[c.decode()]['full_field_coords'] for c in case_name.numpy()],0))\n",
    "    \n",
    "    p_pred, p_ae_pred = get_predictions(models['Pressure'], p_inp, p_target)\n",
    "    p_npred, p_nae_pred, p_ntarget = undo_normalization(normalizers['Pressure'], p_norm_param, p_pred, p_ae_pred, p_target)\n",
    "   \n",
    "    pred_pressures = p_npred[...,0]\n",
    "    ae_pred_pressures = p_nae_pred[...,0]\n",
    "    target_pressures = p_ntarget[...,0]\n",
    "    \n",
    "    compute_metrics(coords, 'Pressure', pred_mapes, pred_unnormalized_mapes, pred_mses, pred_unnormalized_mses,\n",
    "                       p_pred, p_target, p_npred, p_ntarget)\n",
    "    \n",
    "    compute_metrics(coords, 'Pressure', ae_pred_mapes, ae_pred_unnormalized_mapes, ae_pred_mses, ae_pred_unnormalized_mses,\n",
    "                       p_ae_pred, p_target, p_nae_pred, p_ntarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fa6262b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mapes = {k:tf.concat(pred_mapes[k],0) for k in dataset_iterators.keys()}\n",
    "pred_unnormalized_mapes = {k:tf.concat(pred_unnormalized_mapes[k],0) for k in dataset_iterators.keys()}\n",
    "pred_mses = {k:tf.concat(pred_mses[k],0) for k in dataset_iterators.keys()}\n",
    "pred_unnormalized_mses = {k:tf.concat(pred_unnormalized_mses[k],0) for k in dataset_iterators.keys()}\n",
    "\n",
    "ae_pred_mapes = {k:tf.concat(ae_pred_mapes[k],0) for k in dataset_iterators.keys()}\n",
    "ae_pred_unnormalized_mapes = {k:tf.concat(ae_pred_unnormalized_mapes[k],0) for k in dataset_iterators.keys()}\n",
    "ae_pred_mses = {k:tf.concat(ae_pred_mses[k],0) for k in dataset_iterators.keys()}\n",
    "ae_pred_unnormalized_mses = {k:tf.concat(ae_pred_unnormalized_mses[k],0) for k in dataset_iterators.keys()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "892c7f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Overall metrics===\n",
      ">> Pressure Pred\n",
      "MAPE: 24.76392364501953\n",
      "Unnormalized MAPE: 17.91895294189453\n",
      "MSE: 0.3148247301578522\n",
      "Unnormalized MSE: 0.006620199419558048\n",
      "\n",
      ">> Pressure AE Pred\n",
      "MAPE: 13.87522029876709\n",
      "Unnormalized MAPE: 8.794646263122559\n",
      "MSE: 0.0076378704980015755\n",
      "Unnormalized MSE: 0.0007551303715445101\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_metrics(v, s, mapes, unnormalized_mapes, mses, unnormalized_mses):\n",
    "    print(f\">> {v} {s}\")\n",
    "    print(f'MAPE: {tf.reduce_mean(mapes[v]).numpy()}')\n",
    "    print(f'Unnormalized MAPE: {tf.reduce_mean(unnormalized_mapes[v]).numpy()}')\n",
    "    print(f'MSE: {tf.reduce_mean(mses[v]).numpy()}')\n",
    "    print(f'Unnormalized MSE: {tf.reduce_mean(unnormalized_mses[v]).numpy()}')\n",
    "    print('')\n",
    "\n",
    "print('===Overall metrics===')\n",
    "\n",
    "print_metrics('Pressure', 'Pred', pred_mapes, pred_unnormalized_mapes, pred_mses, pred_unnormalized_mses)\n",
    "print_metrics('Pressure', 'AE Pred', ae_pred_mapes, ae_pred_unnormalized_mapes, ae_pred_mses, ae_pred_unnormalized_mses)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
