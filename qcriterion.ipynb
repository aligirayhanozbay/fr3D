{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d385d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import h5py\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.path as mplPath\n",
    "from evtk import hl\n",
    "#from mayavi import mlab\n",
    "#mlab.init_notebook('itk')\n",
    "\n",
    "from fr3D.train.utils import setup_datasets\n",
    "from fr3D.data.utils import get_normalization\n",
    "from fr3D.models import ConvAutoencoder, ConvAutoencoderCGAN, ConvAutoencoderC\n",
    "\n",
    "tf.keras.backend.set_image_data_format('channels_last')\n",
    "\n",
    "def check_in_polygon(pts, vertices):\n",
    "    path = mplPath.Path(vertices)\n",
    "    return path.contains_points(pts)\n",
    "\n",
    "def mape_with_threshold(yp, yt, pcterror_threshold=np.inf, max_magnitude_threshold=0.0, eps=1e-7):\n",
    "    pct_errors = 100*tf.abs((yp-yt)/(eps + yt))\n",
    "    pcterror_mask = pct_errors < pcterror_threshold\n",
    "    max_magnitude_mask = tf.logical_not(tf.abs(yt) < (max_magnitude_threshold*tf.reduce_max(tf.abs(yt))))\n",
    "    filtering_indices = tf.where(tf.logical_and(pcterror_mask, max_magnitude_mask))\n",
    "    filtered_pcterrors = tf.gather_nd(pct_errors, filtering_indices)\n",
    "    return float(tf.reduce_mean(filtered_pcterrors))\n",
    "\n",
    "def get_normalization_type(node_configs):\n",
    "    normalization_spec = {'method': None}\n",
    "    for node in node_configs:\n",
    "        if node['nodetype'] == 'normalize':\n",
    "            normalization_spec = node['normalization_spec']\n",
    "            break\n",
    "    normalizer = get_normalization(**normalization_spec, batch_mode=True)\n",
    "    return normalizer\n",
    "\n",
    "def export_vtk(path, c, **fields):\n",
    "    x = c[...,0].reshape(-1)\n",
    "    y = c[...,1].reshape(-1)\n",
    "    z = c[...,2].reshape(-1)\n",
    "    \n",
    "    innerx = c[:1,...,0].reshape(-1)\n",
    "    innery = c[:1,...,1].reshape(-1)\n",
    "    innerz = c[:1,...,2].reshape(-1)\n",
    "    \n",
    "    data = {}\n",
    "    for f in fields:\n",
    "        data[f] = fields[f].reshape(-1).copy()\n",
    "    hl.pointsToVTK(path, x, y, z, data = data)\n",
    "    hl.pointsToVTK(path+'obstacle', innerx, innery, innerz, data = {'obstacle': np.zeros(c[:1].shape[:-1])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2aba9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Re = 500\n",
    "expt_variables = ['Pressure', 'U', 'V', 'W']\n",
    "\n",
    "dataset_path = f'/fr3D/postprocessed/annulus_64_plane.h5'\n",
    "\n",
    "#ConvAutoencoderC\n",
    "experiment_configs = {expt_variable:f'/fr3D/configs/training_piv/ConvAutoencoderC_{expt_variable}.json' for expt_variable in expt_variables}\n",
    "weights_paths = {expt_variable:f'/storage/weights{Re}piv/ConvAutoencoderC_{expt_variable}_Annulus64/ConvAutoencoderC_{expt_variable}_Annulus64.h5' for expt_variable in expt_variables}\n",
    "\n",
    "\n",
    "\n",
    "datasetf = h5py.File(dataset_path,'r')\n",
    "\n",
    "shuf_buf = 1\n",
    "\n",
    "train_datasets = {}\n",
    "test_datasets = {}\n",
    "sensor_shapes = {}\n",
    "full_field_shapes = {}\n",
    "normalizers = {}\n",
    "\n",
    "for expt_variable in experiment_configs:\n",
    "    config = json.load(open(experiment_configs[expt_variable],'r'))\n",
    "    train_datasets[expt_variable], test_datasets[expt_variable] = setup_datasets(config, dataset_path, shuf_buf, case_names=True, evaluation=True)\n",
    "    sensor_shapes[expt_variable] = train_datasets[expt_variable].element_spec[0][0].shape\n",
    "    full_field_shapes[expt_variable] = train_datasets[expt_variable].element_spec[0][1].shape\n",
    "    normalizers[expt_variable] = get_normalization_type(config['dataset']['node_configurations'])\n",
    "    \n",
    "dataset_iterators = {v: iter(test_datasets[v]) for v in test_datasets}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448ecfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "\n",
    "for v in expt_variables:\n",
    "    config = json.load(open(experiment_configs[v],'r'))\n",
    "    model = ConvAutoencoderC(dense_input_units=sensor_shapes[v][1],\n",
    "                             autoencoder_input_shape=full_field_shapes[v][1:],\n",
    "                             **config['model'])\n",
    "    loss_fn = \"mse\"#tf.keras.losses.get(config['training']['loss'])\n",
    "    model.compile(l_optimizer= tf.keras.optimizers.get(config['training']['l_optimizer']),\n",
    "                  loss=loss_fn,\n",
    "                  optimizer = tf.keras.optimizers.get(config['training']['ae_optimizer']),\n",
    "                  metrics = config['training'].get('metrics', None))\n",
    "    model.load_weights(weights_paths[v])\n",
    "    models[v] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55aa12a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_new = True\n",
    "idx = 0\n",
    "\n",
    "inps = {}\n",
    "targets = {}\n",
    "ntargets = {}\n",
    "norm_params = {}\n",
    "case_names = {}\n",
    "preds = {}\n",
    "npreds = {}\n",
    "ae_preds = {}\n",
    "nae_preds = {}\n",
    "\n",
    "for v in expt_variables:\n",
    "    if get_new:\n",
    "        for _ in range(34):\n",
    "            (inp, target, norm_param), case_name = next(dataset_iterators[v])\n",
    "        pred = models[v](inp,autoencode=False)\n",
    "        ae_pred = models[v](target,autoencode=True)\n",
    "        print(f'MAE error for {v}: {tf.reduce_mean(tf.abs(pred-target))}')\n",
    "        ntarget = normalizers[v].undo(target, norm_param[:,0,:])\n",
    "        npred = normalizers[v].undo(pred, norm_param[:,0,:])\n",
    "        nae_pred = normalizers[v].undo(ae_pred, norm_param[:,0,:])\n",
    "        \n",
    "        print(f'Mean {v} stddev along z axis: {tf.reduce_max(tf.math.reduce_std(ntarget[idx,...,0], axis=-1))}')\n",
    "\n",
    "    inps[v] = inp[idx]\n",
    "    targets['gt'+v] = target[idx,...,0].numpy()\n",
    "    ntargets['gt'+v] = ntarget[idx,...,0].numpy()\n",
    "    norm_params[v] = norm_param[idx]\n",
    "    case_names[v] = case_name[idx].numpy()\n",
    "    preds['pred'+v] = pred[idx,...,0].numpy()\n",
    "    npreds['pred'+v] = npred[idx,...,0].numpy()\n",
    "    ae_preds['aepred'+v] = ae_pred[idx,...,0].numpy()\n",
    "    nae_preds['aepred'+v] = nae_pred[idx,...,0].numpy()\n",
    "\n",
    "\n",
    "assert len(set(case_names.values()))==1\n",
    "case_name_str = list(case_names.values())[0].decode()\n",
    "print(case_name_str)\n",
    "coords = datasetf[case_name_str]['full_field_coords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722c3bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_vtk(f\"/storage/paper/qcrit_planes/qcrit_{case_name_str}\", coords, **ntargets, **nae_preds, **npreds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eeccfcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx=13\n",
    "zsl=32\n",
    "\n",
    "vmax = 1.0\n",
    "vmin = 0.0\n",
    "\n",
    "v = 'U'\n",
    "\n",
    "stdf = 5.0\n",
    "meann = tf.reduce_mean(ntargets['gt'+v][:,:,zsl])\n",
    "stdn = tf.math.reduce_std(ntargets['gt'+v][:,:,zsl])\n",
    "vmaxn = meann+stdf*stdn\n",
    "vminn = meann-stdf*stdn\n",
    "\n",
    "\n",
    "coords = datasetf[case_name_str]['full_field_coords']\n",
    "x = np.reshape(coords[:,:,zsl,0],[-1])\n",
    "y = np.reshape(coords[:,:,zsl,1],[-1])\n",
    "\n",
    "p = np.reshape(preds['pred'+v][:,:,zsl], [-1])\n",
    "pn = np.reshape(npreds['pred'+v][:,:,zsl], [-1])\n",
    "ap = np.reshape(ae_preds['aepred'+v][:,:,zsl], [-1])\n",
    "apn = np.reshape(nae_preds['aepred'+v][:,:,zsl], [-1])\n",
    "t = np.reshape(targets['gt'+v][:,:,zsl], [-1])\n",
    "tn = np.reshape(ntargets['gt'+v][:,:,zsl], [-1])\n",
    "\n",
    "outer_polygon = [(-3,3), (-3,-3), (8,-3), (8,3)]\n",
    "spatialmask = check_in_polygon(np.stack([x,y],-1), outer_polygon)\n",
    "spatialmasked_indices = np.where(spatialmask)[0]\n",
    "x = x[spatialmasked_indices]\n",
    "y = y[spatialmasked_indices]\n",
    "p = p[spatialmasked_indices]\n",
    "pn = pn[spatialmasked_indices]\n",
    "ap = ap[spatialmasked_indices]\n",
    "apn = apn[spatialmasked_indices]\n",
    "t = t[spatialmasked_indices]\n",
    "tn = tn[spatialmasked_indices]\n",
    "\n",
    "polygon_vertices = datasetf[case_name_str]['full_field_coords'][0,:,zsl]\n",
    "\n",
    "#Normalized plots\n",
    "plt.figure()\n",
    "plt.tripcolor(x,y,p, shading='gouraud', cmap='rainbow', vmax = vmax, vmin = vmin)\n",
    "plt.fill(polygon_vertices[...,0], polygon_vertices[...,1], color='purple')\n",
    "plt.colorbar()\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "plt.figure()\n",
    "plt.tripcolor(x,y,ap, shading='gouraud', cmap='rainbow', vmax = vmax, vmin = vmin)\n",
    "plt.fill(polygon_vertices[...,0], polygon_vertices[...,1], color='purple')\n",
    "plt.colorbar()\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "plt.figure()\n",
    "plt.tripcolor(x,y,t, shading='gouraud', cmap='rainbow', vmax = vmax, vmin = vmin)\n",
    "plt.fill(polygon_vertices[...,0], polygon_vertices[...,1], color='purple')\n",
    "plt.colorbar()\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "#Unnormalized plots\n",
    "plt.figure()\n",
    "plt.tripcolor(x,y,pn, shading='gouraud', cmap='rainbow', vmax = vmaxn, vmin = vminn)\n",
    "plt.fill(polygon_vertices[...,0], polygon_vertices[...,1], color='purple')\n",
    "plt.colorbar()\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "plt.figure()\n",
    "plt.tripcolor(x,y,apn, shading='gouraud', cmap='rainbow', vmax = vmaxn, vmin = vminn)\n",
    "plt.fill(polygon_vertices[...,0], polygon_vertices[...,1], color='purple')\n",
    "plt.colorbar()\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "plt.figure()\n",
    "plt.tripcolor(x,y,tn, shading='gouraud', cmap='rainbow', vmax = vmaxn, vmin = vminn)\n",
    "plt.fill(polygon_vertices[...,0], polygon_vertices[...,1], color='purple')\n",
    "plt.colorbar()\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "print(f'{case_name_str} {v} stats:')\n",
    "print(\"==Slice % err==\")\n",
    "print(mape_with_threshold(p,t,pcterror_threshold=200))\n",
    "print(mape_with_threshold(ap,t,pcterror_threshold=200))\n",
    "print(\"==Whole snapshot % err==\")\n",
    "print(mape_with_threshold(preds['pred'+v][idx],targets['gt'+v][idx],pcterror_threshold=200))\n",
    "print(mape_with_threshold(ae_preds['aepred'+v][idx],targets['gt'+v][idx],pcterror_threshold=200))\n",
    "print(\"==Slice % err unnormalized==\")\n",
    "print(mape_with_threshold(pn,tn,pcterror_threshold=200))\n",
    "print(mape_with_threshold(apn,tn,pcterror_threshold=200))\n",
    "print(\"==Whole snapshot % err unnormalized==\")\n",
    "print(mape_with_threshold(npreds['pred'+v][idx],ntargets['gt'+v][idx],pcterror_threshold=200))\n",
    "print(mape_with_threshold(nae_preds['aepred'+v][idx],ntargets['gt'+v][idx],pcterror_threshold=200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b7d1e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
