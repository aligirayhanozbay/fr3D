{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d385d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import h5py\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.path as mplPath\n",
    "from evtk import hl\n",
    "\n",
    "from fr3D.train.utils import setup_datasets\n",
    "from fr3D.data.utils import get_normalization\n",
    "from fr3D.models import ConvAutoencoder, ConvAutoencoderCGAN, ConvAutoencoderC\n",
    "\n",
    "tf.keras.backend.set_image_data_format('channels_last')\n",
    "\n",
    "def check_in_polygon(pts, vertices):\n",
    "    path = mplPath.Path(vertices)\n",
    "    return path.contains_points(pts)\n",
    "\n",
    "def mape_with_threshold(yp, yt, pcterror_threshold=np.inf, max_magnitude_threshold=0.0, eps=1e-7):\n",
    "    pct_errors = 100*tf.abs((yp-yt)/(eps + yt))\n",
    "    pcterror_mask = pct_errors < pcterror_threshold\n",
    "    max_magnitude_mask = tf.logical_not(tf.abs(yt) < (max_magnitude_threshold*tf.reduce_max(tf.abs(yt))))\n",
    "    filtering_indices = tf.where(tf.logical_and(pcterror_mask, max_magnitude_mask))\n",
    "    filtered_pcterrors = tf.gather_nd(pct_errors, filtering_indices)\n",
    "    return float(tf.reduce_mean(filtered_pcterrors))\n",
    "\n",
    "def get_normalization_type(node_configs):\n",
    "    normalization_spec = {'method': None}\n",
    "    for node in node_configs:\n",
    "        if node['nodetype'] == 'normalize':\n",
    "            normalization_spec = node['normalization_spec']\n",
    "            break\n",
    "    normalizer = get_normalization(**normalization_spec, batch_mode=True)\n",
    "    return normalizer\n",
    "\n",
    "def export_vtk(path, c, **fields):\n",
    "    x = c[...,0].reshape(-1)\n",
    "    y = c[...,1].reshape(-1)\n",
    "    z = c[...,2].reshape(-1)\n",
    "    data = {}\n",
    "    for f in fields:\n",
    "        data[f] = fields[f].reshape(-1).copy()\n",
    "    hl.pointsToVTK(path, x, y, z, data = data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c2aba9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training geometries: ('shape_69', 'shape_329', 'shape_46', 'shape_241', 'shape_239', 'shape_228', 'shape_163', 'shape_63', 'shape_137', 'shape_102', 'shape_157', 'shape_197', 'shape_52', 'shape_213', 'shape_84', 'shape_113', 'shape_58', 'shape_59', 'shape_146', 'shape_206', 'shape_331', 'shape_8', 'shape_18', 'shape_236', 'shape_43', 'shape_15', 'shape_23', 'shape_92', 'shape_135', 'shape_50', 'shape_14', 'shape_255', 'shape_75', 'shape_183', 'shape_89', 'shape_12', 'shape_374', 'shape_37', 'shape_218', 'shape_150', 'shape_31', 'shape_216', 'shape_13', 'shape_90', 'shape_182', 'shape_159', 'shape_67', 'shape_170', 'shape_358', 'shape_147', 'shape_17', 'shape_110', 'shape_152', 'shape_224', 'shape_130', 'shape_60', 'shape_126', 'shape_369', 'shape_22', 'shape_83', 'shape_338', 'shape_97', 'shape_33', 'shape_237', 'shape_325', 'shape_39', 'shape_244', 'shape_41', 'shape_355', 'shape_94', 'shape_61', 'shape_234', 'shape_34', 'shape_29', 'shape_95', 'shape_339', 'shape_55', 'shape_21', 'shape_9', 'shape_343')\n",
      "Test geometries: ('shape_361', 'shape_74', 'shape_220', 'shape_186', 'shape_107', 'shape_327', 'shape_160', 'shape_11', 'shape_164', 'shape_78', 'shape_86', 'shape_54', 'shape_76', 'shape_68', 'shape_16', 'shape_346', 'shape_48', 'shape_149', 'shape_88', 'shape_326')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-01 22:07:57.140194: W tensorflow_io/core/kernels/audio_video_mp3_kernels.cc:271] libmp3lame.so.0 or lame functions are not available\n",
      "2022-12-01 22:07:57.140430: I tensorflow_io/core/kernels/cpu_check.cc:128] Your CPU supports instructions that this TensorFlow IO binary was not compiled to use: AVX2 FMA\n",
      "2022-12-01 22:07:57.317518: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-01 22:07:58.190030: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38258 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:01:00.0, compute capability: 8.0\n",
      "2022-12-01 22:07:58.191543: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 38268 MB memory:  -> device: 1, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:43:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training geometries: ('shape_69', 'shape_329', 'shape_46', 'shape_241', 'shape_239', 'shape_228', 'shape_163', 'shape_63', 'shape_137', 'shape_102', 'shape_157', 'shape_197', 'shape_52', 'shape_213', 'shape_84', 'shape_113', 'shape_58', 'shape_59', 'shape_146', 'shape_206', 'shape_331', 'shape_8', 'shape_18', 'shape_236', 'shape_43', 'shape_15', 'shape_23', 'shape_92', 'shape_135', 'shape_50', 'shape_14', 'shape_255', 'shape_75', 'shape_183', 'shape_89', 'shape_12', 'shape_374', 'shape_37', 'shape_218', 'shape_150', 'shape_31', 'shape_216', 'shape_13', 'shape_90', 'shape_182', 'shape_159', 'shape_67', 'shape_170', 'shape_358', 'shape_147', 'shape_17', 'shape_110', 'shape_152', 'shape_224', 'shape_130', 'shape_60', 'shape_126', 'shape_369', 'shape_22', 'shape_83', 'shape_338', 'shape_97', 'shape_33', 'shape_237', 'shape_325', 'shape_39', 'shape_244', 'shape_41', 'shape_355', 'shape_94', 'shape_61', 'shape_234', 'shape_34', 'shape_29', 'shape_95', 'shape_339', 'shape_55', 'shape_21', 'shape_9', 'shape_343')\n",
      "Test geometries: ('shape_361', 'shape_74', 'shape_220', 'shape_186', 'shape_107', 'shape_327', 'shape_160', 'shape_11', 'shape_164', 'shape_78', 'shape_86', 'shape_54', 'shape_76', 'shape_68', 'shape_16', 'shape_346', 'shape_48', 'shape_149', 'shape_88', 'shape_326')\n",
      "Training geometries: ('shape_69', 'shape_329', 'shape_46', 'shape_241', 'shape_239', 'shape_228', 'shape_163', 'shape_63', 'shape_137', 'shape_102', 'shape_157', 'shape_197', 'shape_52', 'shape_213', 'shape_84', 'shape_113', 'shape_58', 'shape_59', 'shape_146', 'shape_206', 'shape_331', 'shape_8', 'shape_18', 'shape_236', 'shape_43', 'shape_15', 'shape_23', 'shape_92', 'shape_135', 'shape_50', 'shape_14', 'shape_255', 'shape_75', 'shape_183', 'shape_89', 'shape_12', 'shape_374', 'shape_37', 'shape_218', 'shape_150', 'shape_31', 'shape_216', 'shape_13', 'shape_90', 'shape_182', 'shape_159', 'shape_67', 'shape_170', 'shape_358', 'shape_147', 'shape_17', 'shape_110', 'shape_152', 'shape_224', 'shape_130', 'shape_60', 'shape_126', 'shape_369', 'shape_22', 'shape_83', 'shape_338', 'shape_97', 'shape_33', 'shape_237', 'shape_325', 'shape_39', 'shape_244', 'shape_41', 'shape_355', 'shape_94', 'shape_61', 'shape_234', 'shape_34', 'shape_29', 'shape_95', 'shape_339', 'shape_55', 'shape_21', 'shape_9', 'shape_343')\n",
      "Test geometries: ('shape_361', 'shape_74', 'shape_220', 'shape_186', 'shape_107', 'shape_327', 'shape_160', 'shape_11', 'shape_164', 'shape_78', 'shape_86', 'shape_54', 'shape_76', 'shape_68', 'shape_16', 'shape_346', 'shape_48', 'shape_149', 'shape_88', 'shape_326')\n"
     ]
    }
   ],
   "source": [
    "Re = 500\n",
    "expt_variables = ['U', 'V', 'W']\n",
    "\n",
    "dataset_path = f'/fr3D/postprocessed/annulus_64_{Re}.h5'\n",
    "\n",
    "#ConvAutoencoderC\n",
    "experiment_configs = {expt_variable:f'/fr3D/configs/training/ConvAutoencoderC_{expt_variable}.json' for expt_variable in expt_variables}\n",
    "weights_paths = {expt_variable:f'/storage/weights{Re}/ConvAutoencoderC_{expt_variable}_Annulus64/ConvAutoencoderC_{expt_variable}_Annulus64.h5' for expt_variable in expt_variables}\n",
    "\n",
    "\n",
    "\n",
    "datasetf = h5py.File(dataset_path,'r')\n",
    "\n",
    "shuf_buf = 1\n",
    "\n",
    "train_datasets = {}\n",
    "test_datasets = {}\n",
    "sensor_shapes = {}\n",
    "full_field_shapes = {}\n",
    "normalizers = {}\n",
    "\n",
    "for expt_variable in experiment_configs:\n",
    "    config = json.load(open(experiment_configs[expt_variable],'r'))\n",
    "    train_datasets[expt_variable], test_datasets[expt_variable] = setup_datasets(config, dataset_path, shuf_buf, case_names=True, evaluation=True)\n",
    "    sensor_shapes[expt_variable] = train_datasets[expt_variable].element_spec[0][0].shape\n",
    "    full_field_shapes[expt_variable] = train_datasets[expt_variable].element_spec[0][1].shape\n",
    "    normalizers[expt_variable] = get_normalization_type(config['dataset']['node_configurations'])\n",
    "    \n",
    "dataset_iterators = {v: iter(test_datasets[v]) for v in test_datasets}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "448ecfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "\n",
    "for v in expt_variables:\n",
    "    config = json.load(open(experiment_configs[v],'r'))\n",
    "    model = ConvAutoencoderC(dense_input_units=sensor_shapes[v][1],\n",
    "                             autoencoder_input_shape=full_field_shapes[v][1:],\n",
    "                             **config['model'])\n",
    "    loss_fn = \"mse\"#tf.keras.losses.get(config['training']['loss'])\n",
    "    model.compile(l_optimizer= tf.keras.optimizers.get(config['training']['l_optimizer']),\n",
    "                  loss=loss_fn,\n",
    "                  optimizer = tf.keras.optimizers.get(config['training']['ae_optimizer']),\n",
    "                  metrics = config['training'].get('metrics', None))\n",
    "    model.load_weights(weights_paths[v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55aa12a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-01 22:08:30.922668: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-12-01 22:08:31.333129: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100\n",
      "2022-12-01 22:08:31.807134: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_383462/2599994667.py\u001b[0m(21)\u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     19 \u001b[0;31m    \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     20 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 21 \u001b[0;31m    \u001b[0minps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     22 \u001b[0;31m    \u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gt'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     23 \u001b[0;31m    \u001b[0mnorm_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnorm_param\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> print(ntarget.shape)\n",
      "(24, 64, 64, 64, 1)\n",
      "ipdb> q\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "\n",
    "inps = {}\n",
    "targets = {}\n",
    "norm_params = {}\n",
    "case_names = {}\n",
    "preds = {}\n",
    "ae_preds = {}\n",
    "\n",
    "for v in expt_variables:    \n",
    "    (inp, target, norm_param), case_name = next(dataset_iterators[v])\n",
    "    pred = model(inp,autoencode=False)\n",
    "    ae_pred = model(target,autoencode=True)\n",
    "    \n",
    "    ntarget = normalizers[v].undo(target, norm_param[:,0,:])\n",
    "    npred = normalizers[v].undo(pred, norm_param[:,0,:])\n",
    "    nae_pred = normalizers[v].undo(ae_pred, norm_param[:,0,:])\n",
    "    \n",
    "    import pdb; pdb.set_trace()\n",
    "\n",
    "    inps[v] = inp[idx]\n",
    "    targets['gt'+v] = ntarget[idx,...,0].numpy()\n",
    "    norm_params[v] = norm_param[idx]\n",
    "    case_names[v] = case_name[idx].numpy()\n",
    "    preds['pred'+v] = npred[idx,...,0].numpy()\n",
    "    ae_preds['aepred'+v] = nae_pred[idx,...,0].numpy()\n",
    "\n",
    "assert len(set(case_names.values()))==1\n",
    "case_name = list(case_names.values())[0]\n",
    "coords = datasetf[case_name]['full_field_coords']\n",
    "\n",
    "export_vtk(f\"/storage/qcrit_{case_name.decode()}\", coords, **targets, **ae_preds, **preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1eeccfcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'U': <fr3D.data.utils.normalization.MinMaxNormalization at 0x7fb70c1840d0>,\n",
       " 'V': <fr3D.data.utils.normalization.MinMaxNormalization at 0x7fb680133fd0>,\n",
       " 'W': <fr3D.data.utils.normalization.MinMaxNormalization at 0x7fb62823b100>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizers[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
