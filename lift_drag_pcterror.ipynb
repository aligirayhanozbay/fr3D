{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78ffe1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import h5py\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.path as mplPath\n",
    "from evtk import hl\n",
    "\n",
    "from fr3D.train.utils import setup_datasets\n",
    "from fr3D.data.utils import get_normalization_type\n",
    "from fr3D.models import ConvAutoencoder, ConvAutoencoderCGAN, ConvAutoencoderC\n",
    "from fr3D.utils import mape_with_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b72df71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training geometries: ('shape_69', 'shape_33', 'shape_37', 'shape_374', 'shape_239', 'shape_224', 'shape_160', 'shape_237', 'shape_135', 'cylinder', 'shape_152', 'shape_186', 'shape_92', 'shape_21', 'shape_58', 'shape_110', 'shape_88', 'shape_59', 'shape_14', 'shape_197', 'shape_55', 'shape_8', 'shape_170', 'shape_234', 'shape_43', 'shape_149', 'shape_228', 'shape_48', 'shape_130', 'shape_90', 'shape_137', 'shape_84', 'shape_361', 'shape_182', 'shape_68', 'shape_113', 'shape_244', 'shape_327', 'shape_216', 'shape_15', 'shape_63', 'shape_213', 'shape_126', 'shape_236', 'shape_50', 'shape_18', 'shape_157', 'shape_89', 'shape_17', 'shape_355', 'shape_29', 'shape_146', 'shape_164', 'shape_11', 'shape_150', 'shape_220', 'shape_13', 'shape_60', 'shape_12', 'shape_369', 'shape_218', 'shape_78', 'shape_331', 'shape_97', 'shape_329', 'shape_31', 'shape_39', 'shape_241', 'shape_41', 'shape_346', 'shape_94', 'shape_61', 'shape_23', 'shape_339', 'shape_255', 'shape_83', 'shape_338', 'shape_54', 'shape_206', 'shape_9')\n",
      "Test geometries: ('shape_34', 'shape_358', 'shape_74', 'shape_22', 'shape_183', 'shape_102', 'shape_326', 'shape_16', 'shape_107', 'shape_163', 'shape_76', 'shape_95', 'shape_52', 'shape_75', 'shape_67', 'shape_159', 'shape_343', 'shape_46', 'shape_147', 'shape_86', 'shape_325')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-20 20:39:20.009554: W tensorflow_io/core/kernels/audio_video_mp3_kernels.cc:271] libmp3lame.so.0 or lame functions are not available\n",
      "2022-12-20 20:39:20.009723: I tensorflow_io/core/kernels/cpu_check.cc:128] Your CPU supports instructions that this TensorFlow IO binary was not compiled to use: AVX2 FMA\n",
      "2022-12-20 20:39:20.329650: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-20 20:39:21.206553: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38278 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:01:00.0, compute capability: 8.0\n",
      "2022-12-20 20:39:21.208088: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 38278 MB memory:  -> device: 1, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:43:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training geometries: ('shape_69', 'shape_33', 'shape_37', 'shape_374', 'shape_239', 'shape_224', 'shape_160', 'shape_237', 'shape_135', 'cylinder', 'shape_152', 'shape_186', 'shape_92', 'shape_21', 'shape_58', 'shape_110', 'shape_88', 'shape_59', 'shape_14', 'shape_197', 'shape_55', 'shape_8', 'shape_170', 'shape_234', 'shape_43', 'shape_149', 'shape_228', 'shape_48', 'shape_130', 'shape_90', 'shape_137', 'shape_84', 'shape_361', 'shape_182', 'shape_68', 'shape_113', 'shape_244', 'shape_327', 'shape_216', 'shape_15', 'shape_63', 'shape_213', 'shape_126', 'shape_236', 'shape_50', 'shape_18', 'shape_157', 'shape_89', 'shape_17', 'shape_355', 'shape_29', 'shape_146', 'shape_164', 'shape_11', 'shape_150', 'shape_220', 'shape_13', 'shape_60', 'shape_12', 'shape_369', 'shape_218', 'shape_78', 'shape_331', 'shape_97', 'shape_329', 'shape_31', 'shape_39', 'shape_241', 'shape_41', 'shape_346', 'shape_94', 'shape_61', 'shape_23', 'shape_339', 'shape_255', 'shape_83', 'shape_338', 'shape_54', 'shape_206', 'shape_9')\n",
      "Test geometries: ('shape_34', 'shape_358', 'shape_74', 'shape_22', 'shape_183', 'shape_102', 'shape_326', 'shape_16', 'shape_107', 'shape_163', 'shape_76', 'shape_95', 'shape_52', 'shape_75', 'shape_67', 'shape_159', 'shape_343', 'shape_46', 'shape_147', 'shape_86', 'shape_325')\n",
      "Training geometries: ('shape_69', 'shape_33', 'shape_37', 'shape_374', 'shape_239', 'shape_224', 'shape_160', 'shape_237', 'shape_135', 'cylinder', 'shape_152', 'shape_186', 'shape_92', 'shape_21', 'shape_58', 'shape_110', 'shape_88', 'shape_59', 'shape_14', 'shape_197', 'shape_55', 'shape_8', 'shape_170', 'shape_234', 'shape_43', 'shape_149', 'shape_228', 'shape_48', 'shape_130', 'shape_90', 'shape_137', 'shape_84', 'shape_361', 'shape_182', 'shape_68', 'shape_113', 'shape_244', 'shape_327', 'shape_216', 'shape_15', 'shape_63', 'shape_213', 'shape_126', 'shape_236', 'shape_50', 'shape_18', 'shape_157', 'shape_89', 'shape_17', 'shape_355', 'shape_29', 'shape_146', 'shape_164', 'shape_11', 'shape_150', 'shape_220', 'shape_13', 'shape_60', 'shape_12', 'shape_369', 'shape_218', 'shape_78', 'shape_331', 'shape_97', 'shape_329', 'shape_31', 'shape_39', 'shape_241', 'shape_41', 'shape_346', 'shape_94', 'shape_61', 'shape_23', 'shape_339', 'shape_255', 'shape_83', 'shape_338', 'shape_54', 'shape_206', 'shape_9')\n",
      "Test geometries: ('shape_34', 'shape_358', 'shape_74', 'shape_22', 'shape_183', 'shape_102', 'shape_326', 'shape_16', 'shape_107', 'shape_163', 'shape_76', 'shape_95', 'shape_52', 'shape_75', 'shape_67', 'shape_159', 'shape_343', 'shape_46', 'shape_147', 'shape_86', 'shape_325')\n",
      "Training geometries: ('shape_69', 'shape_33', 'shape_37', 'shape_374', 'shape_239', 'shape_224', 'shape_160', 'shape_237', 'shape_135', 'cylinder', 'shape_152', 'shape_186', 'shape_92', 'shape_21', 'shape_58', 'shape_110', 'shape_88', 'shape_59', 'shape_14', 'shape_197', 'shape_55', 'shape_8', 'shape_170', 'shape_234', 'shape_43', 'shape_149', 'shape_228', 'shape_48', 'shape_130', 'shape_90', 'shape_137', 'shape_84', 'shape_361', 'shape_182', 'shape_68', 'shape_113', 'shape_244', 'shape_327', 'shape_216', 'shape_15', 'shape_63', 'shape_213', 'shape_126', 'shape_236', 'shape_50', 'shape_18', 'shape_157', 'shape_89', 'shape_17', 'shape_355', 'shape_29', 'shape_146', 'shape_164', 'shape_11', 'shape_150', 'shape_220', 'shape_13', 'shape_60', 'shape_12', 'shape_369', 'shape_218', 'shape_78', 'shape_331', 'shape_97', 'shape_329', 'shape_31', 'shape_39', 'shape_241', 'shape_41', 'shape_346', 'shape_94', 'shape_61', 'shape_23', 'shape_339', 'shape_255', 'shape_83', 'shape_338', 'shape_54', 'shape_206', 'shape_9')\n",
      "Test geometries: ('shape_34', 'shape_358', 'shape_74', 'shape_22', 'shape_183', 'shape_102', 'shape_326', 'shape_16', 'shape_107', 'shape_163', 'shape_76', 'shape_95', 'shape_52', 'shape_75', 'shape_67', 'shape_159', 'shape_343', 'shape_46', 'shape_147', 'shape_86', 'shape_325')\n"
     ]
    }
   ],
   "source": [
    "Re = 500\n",
    "expt_variables = ['Pressure', 'U', 'V', 'W']\n",
    "\n",
    "dataset_path = f'/fr3D/postprocessed/annulus_64.h5'\n",
    "\n",
    "#ConvAutoencoderC\n",
    "experiment_configs = {expt_variable:f'/fr3D/configs/training/ConvAutoencoderC_{expt_variable}.json' for expt_variable in expt_variables}\n",
    "weights_paths = {expt_variable:f'/storage/weights{Re}/ConvAutoencoderC_{expt_variable}_Annulus64/ConvAutoencoderC_{expt_variable}_Annulus64.h5' for expt_variable in expt_variables}\n",
    "\n",
    "\n",
    "\n",
    "datasetf = h5py.File(dataset_path,'r')\n",
    "\n",
    "shuf_buf = 1\n",
    "\n",
    "train_datasets = {}\n",
    "test_datasets = {}\n",
    "sensor_shapes = {}\n",
    "full_field_shapes = {}\n",
    "normalizers = {}\n",
    "\n",
    "for expt_variable in experiment_configs:\n",
    "    config = json.load(open(experiment_configs[expt_variable],'r'))\n",
    "    train_datasets[expt_variable], test_datasets[expt_variable] = setup_datasets(config, dataset_path, shuf_buf, case_names=True, evaluation=True)\n",
    "    sensor_shapes[expt_variable] = train_datasets[expt_variable].element_spec[0][0].shape\n",
    "    full_field_shapes[expt_variable] = train_datasets[expt_variable].element_spec[0][1].shape\n",
    "    normalizers[expt_variable] = get_normalization_type(config['dataset']['node_configurations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48ab5246",
   "metadata": {},
   "outputs": [],
   "source": [
    "def circular_pad(x, coords=True):\n",
    "    if coords:\n",
    "        r = np.zeros((x.shape[0]+2, x.shape[1]+2, x.shape[2]))\n",
    "    else:\n",
    "        r = np.zeros((x.shape[0]+2, x.shape[1]+2))\n",
    "    r[1:-1,1:-1] = x\n",
    "    \n",
    "    r[0,1:-1] = x[-1]\n",
    "    r[-1,1:-1] = x[0]\n",
    "    \n",
    "    if coords:\n",
    "        r[:,0,:2] = r[:,1,:2]\n",
    "        r[:,0,2] = x[0,0,2] - x[0,1,2]\n",
    "        \n",
    "        r[:,-1,:2] = r[:,-2,:2]\n",
    "        r[:,-1,2] = 2*x[0,-1,2] - x[0,-2,2]\n",
    "    return r\n",
    "\n",
    "def compute_surface_normals(ds, normalize=True):\n",
    "    r = {}\n",
    "    for shape in ds:\n",
    "        deltas = ds[shape]['full_field_coords'][1] - ds[shape]['full_field_coords'][0]\n",
    "        if normalize:\n",
    "            normals = deltas / np.linalg.norm(deltas,axis=-1,keepdims=True)\n",
    "        else:\n",
    "            normals = deltas\n",
    "        r[shape] = tf.convert_to_tensor(normals)\n",
    "    return r\n",
    "\n",
    "@tf.function\n",
    "def surface_integrate(vertex_values, vertex_coords):\n",
    "    \n",
    "    padded_coords = tf.concat([vertex_coords, vertex_coords[:1]],0)\n",
    "    padded_values = tf.concat([vertex_values, vertex_values[:1]],0)\n",
    "    \n",
    "    cell_mean_values = tf.nn.conv2d(\n",
    "        tf.convert_to_tensor([[padded_values]]),\n",
    "        tf.constant([[[[0.25]],[[0.25]]],[[[0.25]],[[0.25]]]]),\n",
    "        1, \"VALID\", data_format=\"NCHW\"\n",
    "    )[0,0]\n",
    "    \n",
    "    z_direction_vector = padded_coords[:-1, 1:] - padded_coords[:-1, :-1]\n",
    "    theta_direction_vector = -(padded_coords[1:,:-1] - padded_coords[:-1,:-1])\n",
    "    cell_normals = tf.linalg.cross(theta_direction_vector, z_direction_vector)\n",
    "    cell_areas = tf.linalg.norm(cell_normals, axis=-1)\n",
    "    \n",
    "    return tf.einsum('tzn,tz->n', cell_normals, cell_mean_values)#/(0.5*1.0*(1.0**2)*tf.reduce_sum(cell_areas))\n",
    "\n",
    "@tf.function\n",
    "def compute_shear_stresses(velocities, surface_normals, nu=1.0):\n",
    "    #get surface tangent component of velocity    \n",
    "    R = tf.constant([[0,-1,0],[1,0,0],[0,0,1]],dtype=velocities.dtype)\n",
    "    surface_normals_norm = tf.linalg.norm(surface_normals, axis=-1, keepdims=True)\n",
    "    surface_unit_tangents = tf.einsum('ij,tzj->tzi', R, surface_normals/surface_normals_norm)\n",
    "    velocity_tangent_component = tf.einsum('tzi,tzi->tz', surface_unit_tangents, velocities[1])\n",
    "    \n",
    "    #compute shear stresses from du/dn\n",
    "    tau = nu*velocity_tangent_component/(2*surface_normals_norm[...,0]**2.0)\n",
    "    return tau\n",
    "\n",
    "@tf.function\n",
    "def compute_friction_forces(shear_stresses, vertex_coords):\n",
    "    Fnormal = surface_integrate(shear_stresses, vertex_coords)\n",
    "    return tf.convert_to_tensor([Fnormal[1], -Fnormal[0], Fnormal[2]])\n",
    "\n",
    "# pressure_forces = surface_integrate(\n",
    "#     datasetf['cylinder']['full_field'][600,0,...,0],\n",
    "#     datasetf['cylinder']['full_field_coords'][0,...]\n",
    "# )\n",
    "\n",
    "\n",
    "# print(pressure_forces)\n",
    "# surface_normals_map = compute_surface_normals(datasetf, normalize=False)\n",
    "# tau = compute_shear_stresses(\n",
    "#     datasetf['cylinder']['full_field'][600,...,1:],\n",
    "#     surface_normals_map['cylinder'],\n",
    "#     0.01\n",
    "# )\n",
    "# friction_forces = compute_friction_forces(tau, datasetf['cylinder']['full_field_coords'][0,...])\n",
    "# print(friction_forces)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bc058b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "\n",
    "for v in expt_variables:\n",
    "    config = json.load(open(experiment_configs[v],'r'))\n",
    "    model = ConvAutoencoderC(dense_input_units=sensor_shapes[v][1],\n",
    "                             autoencoder_input_shape=full_field_shapes[v][1:],\n",
    "                             **config['model'])\n",
    "    loss_fn = \"mse\"#tf.keras.losses.get(config['training']['loss'])\n",
    "    model.compile(l_optimizer= tf.keras.optimizers.get(config['training']['l_optimizer']),\n",
    "                  loss=loss_fn,\n",
    "                  optimizer = tf.keras.optimizers.get(config['training']['ae_optimizer']),\n",
    "                  metrics = config['training'].get('metrics', None))\n",
    "    model.load_weights(weights_paths[v])\n",
    "    models[v] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acce48c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-20 20:40:47.991302: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-12-20 20:40:48.389389: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100\n",
      "2022-12-20 20:40:48.827290: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def _surface_integrate_wrap(x):\n",
    "    return surface_integrate(x[0], x[1])\n",
    "\n",
    "@tf.function\n",
    "def _friction_forces(x):\n",
    "    velocities, surface_normals, coords = x\n",
    "    nu = 0.01\n",
    "    tau = compute_shear_stresses(velocities, surface_normals, nu)\n",
    "    friction_forces = compute_friction_forces(tau, coords)\n",
    "    return friction_forces\n",
    "\n",
    "@tf.function\n",
    "def _mape_wıth_threshold_wrap(x):\n",
    "    yp, yt = x\n",
    "    return mape_with_threshold(yp, yt, pcterror_threshold=200)\n",
    "\n",
    "@tf.function\n",
    "def get_predictions(model, inp, target):\n",
    "    return model(inp, autoencode=False), model(target, autoencode=True)\n",
    "\n",
    "@tf.function\n",
    "def flatten_mse(yp, yt):\n",
    "    newshape = tf.convert_to_tensor([tf.shape(yp)[0],-1])\n",
    "    yp_flat = tf.reshape(yp, newshape)\n",
    "    yt_flat = tf.reshape(yt, newshape)\n",
    "    return tf.keras.losses.mse(yp_flat, yt_flat)\n",
    "\n",
    "def undo_normalization(normalizer, norm_param, pred, ae_pred, target):\n",
    "    ntarget = normalizer.undo(target, norm_param[:,0,:])\n",
    "    npred = normalizer.undo(pred, norm_param[:,0,:])\n",
    "    nae_pred = normalizer.undo(ae_pred, norm_param[:,0,:])\n",
    "    return npred, nae_pred, ntarget\n",
    "\n",
    "def compute_metrics(v, mapes, umapes, mses, umses, pred, target, npred, ntarget):\n",
    "    mapes[v].append(tf.map_fn(_mape_wıth_threshold_wrap, (npred, ntarget), fn_output_signature=npred.dtype))\n",
    "    umapes[v].append(tf.map_fn(_mape_wıth_threshold_wrap, (pred, target), fn_output_signature=pred.dtype))\n",
    "    mses[v].append(flatten_mse(npred, ntarget))\n",
    "    umses[v].append(flatten_mse(pred, target))\n",
    "\n",
    "dataset_iterators = {v: iter(test_datasets[v]) for v in test_datasets}\n",
    "surface_normals_map = compute_surface_normals(datasetf, normalize=False)\n",
    "\n",
    "pred_forces = []\n",
    "ae_pred_forces = []\n",
    "target_forces = []\n",
    "\n",
    "pred_mapes = {k:[] for k in dataset_iterators.keys()}\n",
    "pred_unnormalized_mapes = {k:[] for k in dataset_iterators.keys()}\n",
    "pred_mses = {k:[] for k in dataset_iterators.keys()}\n",
    "pred_unnormalized_mses = {k:[] for k in dataset_iterators.keys()}\n",
    "\n",
    "ae_pred_mapes = {k:[] for k in dataset_iterators.keys()}\n",
    "ae_pred_unnormalized_mapes = {k:[] for k in dataset_iterators.keys()}\n",
    "ae_pred_mses = {k:[] for k in dataset_iterators.keys()}\n",
    "ae_pred_unnormalized_mses = {k:[] for k in dataset_iterators.keys()}\n",
    "\n",
    "for pdata, udata, vdata, wdata in zip(*dataset_iterators.values()):\n",
    "    (p_inp, p_target, p_norm_param), case_name = pdata\n",
    "    (u_inp, u_target, u_norm_param), case_name = udata\n",
    "    (v_inp, v_target, v_norm_param), case_name = vdata\n",
    "    (w_inp, w_target, w_norm_param), case_name = wdata\n",
    "    \n",
    "    p_pred, p_ae_pred = get_predictions(models['Pressure'], p_inp, p_target)\n",
    "    u_pred, u_ae_pred = get_predictions(models['U'], u_inp, u_target)\n",
    "    v_pred, v_ae_pred = get_predictions(models['V'], v_inp, v_target)\n",
    "    w_pred, w_ae_pred = get_predictions(models['W'], w_inp, w_target)\n",
    "    \n",
    "    p_npred, p_nae_pred, p_ntarget = undo_normalization(normalizers['Pressure'], p_norm_param, p_pred, p_ae_pred, p_target)\n",
    "    u_npred, u_nae_pred, u_ntarget = undo_normalization(normalizers['U'], u_norm_param, u_pred, u_ae_pred, u_target)\n",
    "    v_npred, v_nae_pred, v_ntarget = undo_normalization(normalizers['V'], v_norm_param, v_pred, v_ae_pred, v_target)\n",
    "    w_npred, w_nae_pred, w_ntarget = undo_normalization(normalizers['W'], w_norm_param, w_pred, w_ae_pred, w_target)\n",
    "    \n",
    "    pred_velocities = tf.concat([u_npred, v_npred, w_pred], -1)\n",
    "    pred_pressures = p_npred[...,0]\n",
    "    \n",
    "    ae_pred_velocities = tf.concat([u_nae_pred, v_nae_pred, w_nae_pred], -1)\n",
    "    ae_pred_pressures = p_nae_pred[...,0]\n",
    "    \n",
    "    target_velocities = tf.concat([u_ntarget, v_ntarget, w_ntarget], -1)\n",
    "    target_pressures = p_ntarget[...,0]\n",
    "    \n",
    "    surface_coords = tf.stack([datasetf[c.decode()]['full_field_coords'][0,...] for c in case_name.numpy()], 0)\n",
    "    surface_normals = tf.stack([surface_normals_map[c.decode()] for c in case_name.numpy()], 0)\n",
    "    \n",
    "    target_pressure_forces = tf.map_fn(_surface_integrate_wrap, (target_pressures[:,0], surface_coords), fn_output_signature=target_pressures.dtype)\n",
    "    pred_pressure_forces = tf.map_fn(_surface_integrate_wrap, (pred_pressures[:,0], surface_coords), fn_output_signature=target_pressures.dtype)\n",
    "    ae_pred_pressure_forces = tf.map_fn(_surface_integrate_wrap, (ae_pred_pressures[:,0], surface_coords), fn_output_signature=target_pressures.dtype)\n",
    "    \n",
    "    target_friction_forces = tf.map_fn(_friction_forces, (target_velocities, surface_normals, surface_coords), fn_output_signature=target_velocities.dtype)\n",
    "    pred_friction_forces = tf.map_fn(_friction_forces, (pred_velocities, surface_normals, surface_coords), fn_output_signature=target_velocities.dtype)\n",
    "    ae_pred_friction_forces = tf.map_fn(_friction_forces, (ae_pred_velocities, surface_normals, surface_coords), fn_output_signature=target_velocities.dtype)\n",
    "    \n",
    "    target_forces.append(target_pressure_forces + target_friction_forces)\n",
    "    pred_forces.append(pred_pressure_forces + pred_friction_forces)\n",
    "    ae_pred_forces.append(ae_pred_pressure_forces + ae_pred_friction_forces)\n",
    "    \n",
    "    compute_metrics('Pressure', pred_mapes, pred_unnormalized_mapes, pred_mses, pred_unnormalized_mses,\n",
    "                       p_pred, p_target, p_npred, p_ntarget)\n",
    "    \n",
    "    compute_metrics('Pressure', ae_pred_mapes, ae_pred_unnormalized_mapes, ae_pred_mses, ae_pred_unnormalized_mses,\n",
    "                       p_ae_pred, p_target, p_nae_pred, p_ntarget)\n",
    "    \n",
    "    compute_metrics('U', pred_mapes, pred_unnormalized_mapes, pred_mses, pred_unnormalized_mses,\n",
    "                       u_pred, u_target, u_npred, u_ntarget)\n",
    "    \n",
    "    compute_metrics('U', ae_pred_mapes, ae_pred_unnormalized_mapes, ae_pred_mses, ae_pred_unnormalized_mses,\n",
    "                       u_ae_pred, u_target, u_nae_pred, u_ntarget)\n",
    "    \n",
    "    compute_metrics('V', pred_mapes, pred_unnormalized_mapes, pred_mses, pred_unnormalized_mses,\n",
    "                       v_pred, v_target, v_npred, v_ntarget)\n",
    "    \n",
    "    compute_metrics('V', ae_pred_mapes, ae_pred_unnormalized_mapes, ae_pred_mses, ae_pred_unnormalized_mses,\n",
    "                       v_ae_pred, v_target, v_nae_pred, v_ntarget)\n",
    "    \n",
    "    compute_metrics('W', pred_mapes, pred_unnormalized_mapes, pred_mses, pred_unnormalized_mses,\n",
    "                       w_pred, w_target, w_npred, w_ntarget)\n",
    "    \n",
    "    compute_metrics('W', ae_pred_mapes, ae_pred_unnormalized_mapes, ae_pred_mses, ae_pred_unnormalized_mses,\n",
    "                       w_ae_pred, w_target, w_nae_pred, w_ntarget)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae15a6a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2889f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([11.139191 14.538002  0.      ], shape=(3,), dtype=float32)\n",
      "tf.Tensor([11.139191 14.538002  0.      ], shape=(3,), dtype=float32)\n",
      "tf.Tensor([ 7.2296185  -0.13040058  0.        ], shape=(3,), dtype=float32)\n",
      "tf.Tensor([ 7.2296185  -0.13040055  0.        ], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def _friction_forces(x):\n",
    "    velocities, surface_normals, coords = x\n",
    "    nu = 0.01\n",
    "    tau = compute_shear_stresses(velocities, surface_normals, nu)\n",
    "    friction_forces = compute_friction_forces(tau, coords)\n",
    "    return friction_forces\n",
    "\n",
    "#sanity check\n",
    "idx = 0\n",
    "target_pressure_forces = tf.map_fn(_surface_integrate_wrap, (target_pressures[:,0], surface_coords), fn_output_signature=target_pressures.dtype)\n",
    "print(target_pressure_forces[idx])\n",
    "\n",
    "pressure_forces = surface_integrate(\n",
    "    datasetf['shape_34']['full_field'][idx,0,...,0],\n",
    "    datasetf['shape_34']['full_field_coords'][0,...]\n",
    ")\n",
    "print(pressure_forces)\n",
    "\n",
    "target_velocities = tf.concat([u_ntarget, v_ntarget, w_ntarget], -1)\n",
    "target_friction_forces = tf.map_fn(_friction_forces, (target_velocities, surface_normals, surface_coords), fn_output_signature=target_velocities.dtype)\n",
    "print(target_friction_forces[idx])\n",
    "\n",
    "tau = compute_shear_stresses(\n",
    "    datasetf['shape_34']['full_field'][idx,...,1:],\n",
    "    surface_normals_map['shape_34'],\n",
    "    0.01\n",
    ")\n",
    "friction_forces = compute_friction_forces(tau, datasetf['shape_34']['full_field_coords'][0,...])\n",
    "print(friction_forces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0176ac3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([10])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@tf.function\n",
    "def flatten_mse(yp, yt):\n",
    "    newshape = tf.convert_to_tensor([tf.shape(yp)[0],-1])\n",
    "    yp_flat = tf.reshape(yp, newshape)\n",
    "    yt_flat = tf.reshape(yt, newshape)\n",
    "    return tf.keras.losses.mse(yp_flat, yt_flat)\n",
    "\n",
    "flatten_mse(tf.random.uniform((10,10,10)), tf.random.uniform((10,10,10))).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
